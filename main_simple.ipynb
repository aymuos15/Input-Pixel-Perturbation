{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from config import CONFIG\n",
    "# from model import ResNet18\n",
    "# from dataset import MNISTLaplacianDataset, MedMNISTDataset\n",
    "\n",
    "# import tqdm\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# import importlib.util\n",
    "\n",
    "# spec = importlib.util.spec_from_file_location(\"test\", \"./test.py\")\n",
    "# test_module = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(test_module)\n",
    "# test = test_module.test  # Use test function/class from test.py\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ################\n",
    "# # Load Dataset #\n",
    "# ################\n",
    "# data = np.load(CONFIG['data_path'])\n",
    "# train_images, train_labels = data['train_images'], data['train_labels']\n",
    "# test_images, test_labels = data['test_images'], data['test_labels']\n",
    "\n",
    "# ###################################\n",
    "# # Train function to compute alpha #\n",
    "# ###################################\n",
    "# def lr_lambda(epoch):\n",
    "#     initial_lr = 0.001  # Initial learning rate\n",
    "#     if epoch < 50:\n",
    "#         return initial_lr / initial_lr  # Learning rate remains 0.001\n",
    "#     elif epoch < 75:\n",
    "#         return 0.1 * initial_lr / initial_lr  # Delay learning rate to 0.0001 after 50 epochs\n",
    "#     else:\n",
    "#         return 0.01 * initial_lr / initial_lr  # Delay learning rate to 0.00001 after 75 epochs\n",
    "    \n",
    "# def train(model, train_loader, task, epsilon_p):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_targets = []\n",
    "#     all_outputs = []\n",
    "    \n",
    "#     # For tracking alpha values\n",
    "#     epoch_alpha_values = []\n",
    "#     batch_alpha_values = []\n",
    "    \n",
    "#     if task == \"multi-label, binary-class\":\n",
    "#         criterion = nn.BCEWithLogitsLoss()\n",
    "#     else:\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#     criterion.to(CONFIG['device'])\n",
    "\n",
    "#     lr = 0.001\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "#     for epoch in tqdm.tqdm(range(CONFIG['num_epochs'])):\n",
    "#         epoch_gradients = []\n",
    "        \n",
    "#         for inputs, targets in train_loader:\n",
    "#             inputs, targets = inputs.float().to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             if task == 'multi-label, binary-class':\n",
    "#                 targets = targets.to(torch.float32)\n",
    "#                 loss = criterion(outputs, targets)\n",
    "                \n",
    "#                 # Calculate accuracy for multi-label\n",
    "#                 predicted = (outputs > 0.5).float()\n",
    "#                 correct += (predicted == targets).all(dim=1).sum().item()\n",
    "#             else:\n",
    "#                 targets = targets.squeeze().long()\n",
    "#                 loss = criterion(outputs, targets)\n",
    "                \n",
    "#                 # Calculate accuracy for standard classification\n",
    "#                 _, predicted = outputs.max(1)\n",
    "#                 correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#             # Compute loss gradient\n",
    "#             loss.backward()\n",
    "            \n",
    "#             # Compute alpha as the maximum L2 norm of gradients per sample\n",
    "#             batch_gradients = []\n",
    "#             for i in range(inputs.size(0)):\n",
    "#                 # Initialize sum of squared gradients for this sample\n",
    "#                 sample_grad_squared_sum = 0\n",
    "                \n",
    "#                 # Accumulate gradients from all parameters\n",
    "#                 for param in model.parameters():\n",
    "#                     if param.grad is not None:\n",
    "#                         # Accumulate squared gradient norm\n",
    "#                         sample_grad_squared_sum += param.grad.norm(2).item() ** 2\n",
    "                \n",
    "#                 # Store the square root (L2 norm)\n",
    "#                 batch_gradients.append(np.sqrt(sample_grad_squared_sum))\n",
    "            \n",
    "#             # Find max gradient norm in this batch\n",
    "#             if batch_gradients:\n",
    "#                 batch_max_grad = max(batch_gradients)\n",
    "#                 batch_alpha_values.append(batch_max_grad)\n",
    "#                 epoch_gradients.append(batch_max_grad)\n",
    "            \n",
    "#             total += targets.size(0)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Store targets and outputs for AUC calculation\n",
    "#             all_targets.extend(targets.cpu().numpy())\n",
    "#             all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "        \n",
    "#         # Find max gradient norm in this epoch\n",
    "#         if epoch_gradients:\n",
    "#             epoch_max_grad = max(epoch_gradients)\n",
    "#             epoch_alpha_values.append((epoch, epoch_max_grad))\n",
    "        \n",
    "#         scheduler.step()\n",
    "\n",
    "#     # Compute final alpha as maximum gradient norm across all batches\n",
    "#     alpha = max(batch_alpha_values) if batch_alpha_values else 0\n",
    "    \n",
    "#     return model, alpha, epoch_alpha_values\n",
    "\n",
    "# ########\n",
    "# # Main #\n",
    "# ########\n",
    "# # Define transformation\n",
    "# data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# # Initialize dictionaries to store results\n",
    "# results = defaultdict(lambda: defaultdict(list))\n",
    "# classwise_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "# alpha_results = {}\n",
    "\n",
    "# # epsilon_values = [0.01, 0.1, 1.0, 10, 100, 1000]\n",
    "# epsilon_values = [0.1, 1.0, 10]\n",
    "# num_folds = CONFIG['num_folds']\n",
    "\n",
    "# # For plotting\n",
    "# alpha_vs_epsilon = []\n",
    "# epoch_alpha_data = defaultdict(list)\n",
    "\n",
    "# for epsilon_p in epsilon_values:\n",
    "#     print(f\"Running for epsilon_p: {epsilon_p}\")\n",
    "            \n",
    "#     # Create datasets\n",
    "#     train_dataset = MNISTLaplacianDataset(train_images, train_labels, epsilon_p=epsilon_p, transform=data_transform)\n",
    "#     test_dataset = MedMNISTDataset(test_images, test_labels, transform=data_transform)\n",
    "\n",
    "#     # Create data loaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "#     # Initialize model\n",
    "#     model = ResNet18(CONFIG['num_channels'], CONFIG['num_classes']).to(CONFIG['device'])\n",
    "\n",
    "#     # Train and test the model\n",
    "#     train_model, alpha, epoch_alpha_values = train(model, train_loader, CONFIG['task'], epsilon_p)\n",
    "#     acc, auc, class_acc = test(train_model, test_loader, CONFIG['task'])\n",
    "    \n",
    "#     # Store results\n",
    "#     alpha_results[epsilon_p] = alpha\n",
    "#     alpha_vs_epsilon.append((epsilon_p, alpha))\n",
    "#     epoch_alpha_data[epsilon_p] = epoch_alpha_values\n",
    "    \n",
    "#     results[epsilon_p]['acc'] = acc\n",
    "#     results[epsilon_p]['auc'] = auc\n",
    "    \n",
    "#     for cls, acc_val in class_acc.items():\n",
    "#         classwise_results[epsilon_p]['acc'][cls] = acc_val\n",
    "\n",
    "#     # Print results for this epsilon\n",
    "#     print(f\"\\nResults for epsilon_p = {epsilon_p}:\")\n",
    "#     print(f\"Alpha (gradient bound): {alpha:.4f}\")\n",
    "#     print(f\"Accuracy: {acc:.4f}\")\n",
    "#     print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "#     del model, train_model, train_dataset, test_dataset, train_loader, test_loader\n",
    "\n",
    "# # Create and display alpha vs epsilon plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# eps_values, alpha_values = zip(*alpha_vs_epsilon)\n",
    "# plt.plot(eps_values, alpha_values, marker='o')\n",
    "# plt.xscale('log')  # Log scale for epsilon values\n",
    "# plt.xlabel('Epsilon (ε)')\n",
    "# plt.ylabel('Alpha (α) - Gradient Bound')\n",
    "# plt.title('Gradient Bound (α) vs. Noise Parameter (ε)')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Create and display alpha across epochs plot\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# for eps, epoch_data in epoch_alpha_data.items():\n",
    "#     if epoch_data:\n",
    "#         epochs, alphas = zip(*epoch_data)\n",
    "#         plt.plot(epochs, alphas, marker='.', label=f'ε = {eps}')\n",
    "\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Alpha (α) - Gradient Bound')\n",
    "# plt.title('Gradient Bound (α) Across Training Epochs')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Print comprehensive results table\n",
    "# results_table = pd.DataFrame()\n",
    "# for eps in epsilon_values:\n",
    "#     row = {\n",
    "#         'epsilon_p': eps,\n",
    "#         'alpha': alpha_results[eps],\n",
    "#         'accuracy': results[eps]['acc'],\n",
    "#         'auc': results[eps]['auc']\n",
    "#     }\n",
    "#     results_table = pd.concat([results_table, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# print(\"\\nComprehensive Results Table:\")\n",
    "# print(results_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
