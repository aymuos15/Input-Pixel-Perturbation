{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum L2 Norm: 6324.39208984375\n",
      "Epsilon: 0.01, Max L2 Norm: 24.798959732055664\n",
      "Epsilon: 0.1, Max L2 Norm: 24.77741813659668\n",
      "Epsilon: 1, Max L2 Norm: 24.757051467895508\n",
      "Epsilon: 10, Max L2 Norm: 24.77016258239746\n",
      "Epsilon: 100, Max L2 Norm: 24.74211311340332\n",
      "Epsilon: 1000, Max L2 Norm: 24.745718002319336\n",
      "Epsilon: 10000, Max L2 Norm: 24.74977684020996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24.798959732055664,\n",
       " 24.77741813659668,\n",
       " 24.757051467895508,\n",
       " 24.77016258239746,\n",
       " 24.74211311340332,\n",
       " 24.745718002319336,\n",
       " 24.74977684020996]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from config import CONFIG\n",
    "from dataset import MNISTLaplacianDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load data using numpy for .npz files\n",
    "data = np.load(CONFIG['data_path'])\n",
    "train_images = data['train_images']\n",
    "train_labels = data['train_labels']\n",
    "test_images = data['test_images']\n",
    "test_labels = data['test_labels']\n",
    "\n",
    "# Transform to convert numpy arrays to torch tensors\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "train_images_tensor = torch.from_numpy(train_images).float()\n",
    "train_labels_tensor = torch.from_numpy(train_labels).long()\n",
    "test_images_tensor = torch.from_numpy(test_images).float()\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "\n",
    "# Calculate original L2 norm using torch\n",
    "# Reshape if needed (assuming images are [N, H, W] and need to be flattened to [N, H*W])\n",
    "if len(train_images_tensor.shape) == 3:  # [N, H, W]\n",
    "    train_norms = torch.norm(train_images_tensor.reshape(train_images_tensor.size(0), -1), dim=1)\n",
    "    test_norms = torch.norm(test_images_tensor.reshape(test_images_tensor.size(0), -1), dim=1)\n",
    "else:  # Already in appropriate format\n",
    "    train_norms = torch.norm(train_images_tensor, dim=(1, 2))\n",
    "    test_norms = torch.norm(test_images_tensor, dim=(1, 2))\n",
    "\n",
    "# Find maximum L2 norm\n",
    "max_l2_norm = torch.max(torch.max(train_norms), torch.max(test_norms))\n",
    "print(f\"Maximum L2 Norm: {max_l2_norm.item()}\")\n",
    "\n",
    "# Test various epsilon values\n",
    "epsilon_values = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "max_l2_norms = []\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    # Create dataset with Laplacian noise\n",
    "    train_dataset = MNISTLaplacianDataset(train_images, train_labels, epsilon_p=epsilon, transform=data_transform)\n",
    "    \n",
    "    # Create a DataLoader to efficiently process the dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "    \n",
    "    # Get all images directly from the dataset\n",
    "    for images_batch, _ in train_loader:\n",
    "        # Calculate L2 norms\n",
    "        batch_norms = torch.norm(images_batch.reshape(images_batch.size(0), -1), dim=1)\n",
    "        max_l2_norm = torch.max(batch_norms).item()\n",
    "        max_l2_norms.append(max_l2_norm)\n",
    "        print(f\"Epsilon: {epsilon}, Max L2 Norm: {max_l2_norm}\")\n",
    "        break  # Only need one batch since we're loading the entire dataset\n",
    "\n",
    "max_l2_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset_statistics(dataloaders, image_datasets, class_names):\n",
    "    \"\"\"\n",
    "    Perform full EDA on dataset including:\n",
    "    - Class distribution\n",
    "    - Batch visualization\n",
    "    - Before and after transformations\n",
    "    - Pixel intensity distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # 1️⃣ Class Distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    class_counts = [len(image_datasets['train'].samples) for _ in class_names]\n",
    "    ax.bar(class_names, class_counts, color='skyblue')\n",
    "    ax.set_title('Class Distribution in Training Set')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Number of Images')\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # 2️⃣ Visualizing a Batch of Images\n",
    "    def imshow(inp, title=None):\n",
    "        \"\"\"Imshow for Tensors\"\"\"\n",
    "        inp = inp.numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        inp = std * inp + mean  # Unnormalize\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "        plt.imshow(inp)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "    images, labels = next(iter(dataloaders['train']))\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(12, 5))\n",
    "    for img, lbl, ax in zip(images, labels, axes):\n",
    "        imshow(img, title=class_names[lbl])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # 3️⃣ Before and After Transformations\n",
    "    transform_before = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    transform_after = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    img_path = image_datasets['train'].samples[0][0]\n",
    "    img = datasets.folder.default_loader(img_path)\n",
    "\n",
    "    img_before = transform_before(img)\n",
    "    img_after = transform_after(img)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axes[0].imshow(img_before.permute(1, 2, 0))\n",
    "    axes[0].set_title(\"Before Augmentation\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    img_after_np = img_after.numpy().transpose(1, 2, 0)\n",
    "    img_after_np = np.clip(img_after_np * 0.225 + 0.456, 0, 1)  # Unnormalize\n",
    "    axes[1].imshow(img_after_np)\n",
    "    axes[1].set_title(\"After Augmentation\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # 4️⃣ Pixel Intensity Distribution\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Before Normalization\n",
    "    img_flat = img_before.numpy().flatten()\n",
    "    ax[0].hist(img_flat, bins=50, color='blue', alpha=0.6)\n",
    "    ax[0].set_title(\"Pixel Intensity Before Normalization\")\n",
    "    \n",
    "    # After Normalization\n",
    "    img_flat_after = img_after.numpy().flatten()\n",
    "    ax[1].hist(img_flat_after, bins=50, color='red', alpha=0.6)\n",
    "    ax[1].set_title(\"Pixel Intensity After Normalization\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call function to visualize dataset statistics\n",
    "visualize_dataset_statistics(dataloaders, image_datasets, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
